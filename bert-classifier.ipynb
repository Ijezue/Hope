{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11753531,"sourceType":"datasetVersion","datasetId":7378724}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/data-files/en_train.csv')\ntest_df = pd.read_csv('/kaggle/input/data-files/en_dev.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T01:56:02.277347Z","iopub.execute_input":"2025-05-10T01:56:02.278014Z","iopub.status.idle":"2025-05-10T01:56:02.319541Z","shell.execute_reply.started":"2025-05-10T01:56:02.277988Z","shell.execute_reply":"2025-05-10T01:56:02.318784Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# BERT Tokenization\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ndef encode_texts(texts, max_length=128):\n    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n\n# Split training data into train and validation sets\ntrain_texts, val_texts, train_binary_labels, val_binary_labels = train_test_split(\n    train_df['text'], train_df['binary'], test_size=0.2, random_state=42\n)\ntrain_multi_labels, val_multi_labels = train_test_split(\n    train_df['multiclass'], test_size=0.2, random_state=42\n)\n\ntrain_encodings = encode_texts(train_texts)\nval_encodings = encode_texts(val_texts)\ntest_encodings = encode_texts(test_df['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T01:56:22.804936Z","iopub.execute_input":"2025-05-10T01:56:22.805212Z","iopub.status.idle":"2025-05-10T01:56:28.998268Z","shell.execute_reply.started":"2025-05-10T01:56:22.805189Z","shell.execute_reply":"2025-05-10T01:56:28.997448Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b688e192c4fc4c7aa9d4809df4b19349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeaf2640cc504d229239ccf5139d7aa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c56919ff396b4d39bc822118484239e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78cabc3c3a4c4da69224419e416c0435"}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1746842187.156666      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Label mapping\nbinary_label_map = {'Not Hope': 0, 'Hope': 1}\nmulti_label_map = {'Not Hope': 0, 'Generalized Hope': 1, 'Realistic Hope': 2, 'Unrealistic Hope': 3, 'Sarcasm': 4}\n\ny_train_binary = train_binary_labels.map(binary_label_map)\ny_val_binary = val_binary_labels.map(binary_label_map)\ny_test_binary = test_df['binary'].map(binary_label_map)\ny_train_multi = train_multi_labels.map(multi_label_map)\ny_val_multi = val_multi_labels.map(multi_label_map)\ny_test_multi = test_df['multiclass'].map(multi_label_map)\n\n# Binary Model\nmodel_binary = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel_binary.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T01:56:51.337480Z","iopub.execute_input":"2025-05-10T01:56:51.337802Z","iopub.status.idle":"2025-05-10T01:56:54.985490Z","shell.execute_reply.started":"2025-05-10T01:56:51.337781Z","shell.execute_reply":"2025-05-10T01:56:54.984906Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c2bde1dd934557b320285850af1304"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Callback to save the best model\ncheckpoint_binary = tf.keras.callbacks.ModelCheckpoint(\n    '/kaggle/working/bert_binary_model',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    save_format='tf'\n)\n\nmodel_binary.fit(\n    [train_encodings['input_ids'], train_encodings['attention_mask']],\n    y_train_binary,\n    validation_data=([val_encodings['input_ids'], val_encodings['attention_mask']], y_val_binary),\n    epochs=3,  \n    batch_size=8,\n    callbacks=[checkpoint_binary],\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T01:57:13.632787Z","iopub.execute_input":"2025-05-10T01:57:13.633640Z","iopub.status.idle":"2025-05-10T02:03:53.802048Z","shell.execute_reply.started":"2025-05-10T01:57:13.633593Z","shell.execute_reply":"2025-05-10T02:03:53.801475Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n524/524 [==============================] - 171s 277ms/step - loss: 0.4177 - accuracy: 0.8060 - val_loss: 0.3014 - val_accuracy: 0.8701\nEpoch 2/3\n524/524 [==============================] - 143s 274ms/step - loss: 0.2201 - accuracy: 0.9130 - val_loss: 0.3149 - val_accuracy: 0.8730\nEpoch 3/3\n524/524 [==============================] - 86s 164ms/step - loss: 0.1061 - accuracy: 0.9627 - val_loss: 0.3281 - val_accuracy: 0.8691\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7b148e3c2e50>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Evaluate on test set\nbinary_pred = model_binary.predict([test_encodings['input_ids'], test_encodings['attention_mask']])\nbinary_pred_labels = tf.argmax(binary_pred.logits, axis=1)\nbinary_acc = accuracy_score(y_test_binary, binary_pred_labels)\n\n# Calculate weighted metrics for binary model\nbinary_w_prec, binary_w_rec, binary_w_f1, _ = precision_recall_fscore_support(y_test_binary, binary_pred_labels, average='weighted')\nbinary_m_prec, binary_m_rec, binary_m_f1, _ = precision_recall_fscore_support(y_test_binary, binary_pred_labels, average='macro')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T02:05:24.599643Z","iopub.execute_input":"2025-05-10T02:05:24.600304Z","iopub.status.idle":"2025-05-10T02:05:38.015193Z","shell.execute_reply.started":"2025-05-10T02:05:24.600282Z","shell.execute_reply":"2025-05-10T02:05:38.014657Z"}},"outputs":[{"name":"stdout","text":"60/60 [==============================] - 13s 162ms/step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Multiclass Model\nmodel_multi = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\nmodel_multi.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n# Callback to save the best model\ncheckpoint_multi = tf.keras.callbacks.ModelCheckpoint(\n    '/kaggle/working/bert_multi_model',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    save_format='tf'\n)\n\nmodel_multi.fit(\n    [train_encodings['input_ids'], train_encodings['attention_mask']],\n    y_train_multi,\n    validation_data=([val_encodings['input_ids'], val_encodings['attention_mask']], y_val_multi),\n    epochs=3,  \n    batch_size=8,\n    callbacks=[checkpoint_multi],\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T02:06:15.186657Z","iopub.execute_input":"2025-05-10T02:06:15.187369Z","iopub.status.idle":"2025-05-10T02:12:50.763023Z","shell.execute_reply.started":"2025-05-10T02:06:15.187347Z","shell.execute_reply":"2025-05-10T02:12:50.762439Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n524/524 [==============================] - 166s 276ms/step - loss: 1.0867 - accuracy: 0.5552 - val_loss: 0.6923 - val_accuracy: 0.7545\nEpoch 2/3\n524/524 [==============================] - 143s 273ms/step - loss: 0.6102 - accuracy: 0.7783 - val_loss: 0.5792 - val_accuracy: 0.7775\nEpoch 3/3\n524/524 [==============================] - 86s 163ms/step - loss: 0.3095 - accuracy: 0.8989 - val_loss: 0.6866 - val_accuracy: 0.7612\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7b12e45a6750>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Evaluate on test set\nmulti_pred = model_multi.predict([test_encodings['input_ids'], test_encodings['attention_mask']])\nmulti_pred_labels = tf.argmax(multi_pred.logits, axis=1)\nmulti_acc = accuracy_score(y_test_multi, multi_pred_labels)\n\n# Calculate weighted and macro metrics for multiclass model\nmulti_w_prec, multi_w_rec, multi_w_f1, _ = precision_recall_fscore_support(y_test_multi, multi_pred_labels, average='weighted')\nmulti_m_prec, multi_m_rec, multi_m_f1, _ = precision_recall_fscore_support(y_test_multi, multi_pred_labels, average='macro')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T02:16:58.664899Z","iopub.execute_input":"2025-05-10T02:16:58.665664Z","iopub.status.idle":"2025-05-10T02:17:12.080765Z","shell.execute_reply.started":"2025-05-10T02:16:58.665635Z","shell.execute_reply":"2025-05-10T02:17:12.080048Z"}},"outputs":[{"name":"stdout","text":"60/60 [==============================] - 13s 162ms/step\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Save tokenizer\ntokenizer.save_pretrained('/kaggle/working/bert_tokenizer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T02:17:18.143599Z","iopub.execute_input":"2025-05-10T02:17:18.144099Z","iopub.status.idle":"2025-05-10T02:17:18.166188Z","shell.execute_reply.started":"2025-05-10T02:17:18.144077Z","shell.execute_reply":"2025-05-10T02:17:18.165578Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/bert_tokenizer/tokenizer_config.json',\n '/kaggle/working/bert_tokenizer/special_tokens_map.json',\n '/kaggle/working/bert_tokenizer/vocab.txt',\n '/kaggle/working/bert_tokenizer/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Print results\nprint(f\"Binary Accuracy: {binary_acc:.4f}\")\nprint(f\"Binary Weighted Precision: {binary_w_prec:.4f}\")\nprint(f\"Binary Weighted Recall: {binary_w_rec:.4f}\")\nprint(f\"Binary Weighted F1: {binary_w_f1:.4f}\")\nprint(f\"Binary Macro Precision: {binary_m_prec:.4f}\")\nprint(f\"Binary Macro Recall: {binary_m_rec:.4f}\")\nprint(f\"Binary Macro F1: {binary_m_f1:.4f}\")\nprint(f\"Multiclass Accuracy: {multi_acc:.4f}\")\nprint(f\"Multiclass Weighted Precision: {multi_w_prec:.4f}\")\nprint(f\"Multiclass Weighted Recall: {multi_w_rec:.4f}\")\nprint(f\"Multiclass Weighted F1: {multi_w_f1:.4f}\")\nprint(f\"Multiclass Macro Precision: {multi_m_prec:.4f}\")\nprint(f\"Multiclass Macro Recall: {multi_m_rec:.4f}\")\nprint(f\"Multiclass Macro F1: {multi_m_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T02:18:13.074352Z","iopub.execute_input":"2025-05-10T02:18:13.075048Z","iopub.status.idle":"2025-05-10T02:18:13.080431Z","shell.execute_reply.started":"2025-05-10T02:18:13.075023Z","shell.execute_reply":"2025-05-10T02:18:13.079561Z"}},"outputs":[{"name":"stdout","text":"Binary Accuracy: 0.8365\nBinary Weighted Precision: 0.8420\nBinary Weighted Recall: 0.8365\nBinary Weighted F1: 0.8365\nBinary Macro Precision: 0.8393\nBinary Macro Recall: 0.8391\nBinary Macro F1: 0.8365\nMulticlass Accuracy: 0.7487\nMulticlass Weighted Precision: 0.7755\nMulticlass Weighted Recall: 0.7487\nMulticlass Weighted F1: 0.7517\nMulticlass Macro Precision: 0.7136\nMulticlass Macro Recall: 0.7530\nMulticlass Macro F1: 0.7190\n","output_type":"stream"}],"execution_count":12}]}